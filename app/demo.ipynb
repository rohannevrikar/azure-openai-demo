{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a look at embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to see what the embeddings look like for the corresponding input data. With the installation of SentenceTransformer lib, the embedding model all-MiniLM-L6-v2 also has been downloaded. \\\n",
    "all-MiniLM-L6-v2 maps text to 384 dimensional vector. \\\n",
    "We can also use Azure OpenAI's text-embedding-ada-002 model for this purpose.\\\n",
    "I'm using this locally as the model size is comparatively small (80mb) and could help with cost-memory-performance trade-off decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(\"This is a sample text\")\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store embeddings in an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in input_data:\n",
    "    embeddings = model.encode(data['content'])\n",
    "    data['contentVector'] = embeddings.tolist()\n",
    "\n",
    "with open(\"output.json\", \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLM for generating human-friendly response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not just document files, but we can work with webpages as well.\\\n",
    "In this example, we will try to ask questions about The Digital Personal Data Protection Bill introduced by the Government of India in 2023.\\\n",
    "ChatGPT was trained on data prior to 2021. Using the code/approach below, we can make use of LLMs against the data that we desire.\\\n",
    "Let's start with importing the required modules and environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohannevrikar/Desktop/repos/azure-openai-demo/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a webpage using Langchain's WebBaseLoader, and split it into chunks of smaller sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://prsindia.org/billtrack/digital-personal-data-protection-bill-2023\")\n",
    "\n",
    "\n",
    "data = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings\n",
    "We can generate embeddings using either the model available to us locally (all-MiniLM-L6-v2) or use Azure OpenAI's embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") for using local model\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    openai_api_type=\"azure\", \n",
    "    deployment=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating embeddings, we need a way to store, retrieve, and compare embeddings.\\\n",
    "Vector databases do just that.\\\n",
    "Chroma is a popular open source vector database. These two lines of code generate and store embeddings in local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriever helps with extracting document from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Azure OpenAI LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "    model_name=\"gpt-35-turbo\",\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_type=\"azure\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA does QA over the retriever based on a query provided by the user using the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the duties of data fiducaries in the Digital Personal Data Protection Bill?',\n",
       " 'result': 'The Digital Personal Data Protection Bill, 2023 requires data fiduciaries to maintain the accuracy of data, keep data secure, and delete data once its purpose has been met. They must make reasonable efforts to ensure the accuracy and completeness of data, build reasonable security safeguards to prevent a data breach, inform the Data Protection Board of India and affected persons in the event of a breach, and erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). However, in case of government entities, storage limitation and the right of the data principal to erasure will not apply.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the duties of data fiducaries in the Digital Personal Data Protection Bill?\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever)\n",
    "qa_chain({\"query\": query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
